{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import time, os\n",
    "\n",
    "pd.set_option('display.max_rows', 308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile pdfs into dataframe\n",
    "columns = ['Circuit', 'Tiers', 'Start Date', 'Key Communities', 'End Date']\n",
    "data = pd.DataFrame(columns = columns)\n",
    "\n",
    "data = pd.concat([\n",
    "    pd.read_csv(\"PSPS - 06.21.19.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 10.05.19.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 10.09.19.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 10.10.19.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 10.23.19.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 10.26.19.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 10.31.18.csv\", names = columns),\n",
    "    pd.read_csv(\"PSPS - 11.20.19.csv\", names = columns)\n",
    "], ignore_index = True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import pandas.io.sql as pd_sql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop columns\n",
    "data = data.drop(columns = ['Circuit', 'Tiers'])\n",
    "\n",
    "#limit key communities to one\n",
    "data = data.applymap(lambda x: x.split('\\r', 1)[0])\n",
    "data = data.applymap(lambda x: x.split(',', 1)[0])\n",
    "#data['Circuit'] = data['Circuit'].apply(lambda x: re.sub(r'[^a-zA-Z]', '', x))\n",
    "\n",
    "#remove transmission line data\n",
    "transmission_entries = data[data['Key Communities'] == 'Transmission Line'].index\n",
    "data.drop(transmission_entries, inplace=True)\n",
    "\n",
    "#remove rows with invalid dates\n",
    "data['Start Date'] = data['Start Date'].replace(r'^([^0-9]*)$', np.nan, regex=True)\n",
    "data = data.dropna()\n",
    "\n",
    "#remove timestamps\n",
    "data['Start Date'] = data['Start Date'].apply(lambda x: x.split(' ', 1)[0])\n",
    "data['End Date'] = data['End Date'].apply(lambda x: x.split(' ', 1)[0])\n",
    "\n",
    "#remove rows without full dates\n",
    "data = data[data['Start Date'].str.contains('/')]\n",
    "\n",
    "#format date column\n",
    "data['Start Date'] = data['Start Date'].astype('datetime64')\n",
    "data['End Date'] = data['End Date'].astype('datetime64') \n",
    "\n",
    "#proper capitalization on key communities\n",
    "data['Key Communities'] = data['Key Communities'].apply(lambda x: x.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE TABLE locations (\n",
    "# Index int NOT NULL,\n",
    "# \"Key Communities\" text NOT NULL,\n",
    "# \"Latitude\" float NOT NULL,\n",
    "# \"Longitude\" float NOT NULL\n",
    "# );\n",
    "\n",
    "connection_args = {\n",
    "    'host': 'localhost',  # We are connecting to our _local_ version of psql\n",
    "    'dbname': 'project3',    # DB that we are connecting to\n",
    "    'port': 5432          # port for psql\n",
    "}\n",
    "\n",
    "connection = pg.connect(**connection_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM locations;\"\n",
    "\n",
    "locations = pd_sql.read_sql(query, connection)\n",
    "locations = locations.drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(locations, on = 'Key Communities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Shutoff']=True\n",
    "\n",
    "columns = ['Key Communities', 'Start Date', 'Shutoff', 'Latitude', 'Longitude', 'End Date']\n",
    "data = data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data,open('data','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open('data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## non-target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_date_set = set()\n",
    "\n",
    "def entered_data(row):\n",
    "    index = row.name\n",
    "    comm = row['Key Communities']\n",
    "    date = row['Start Date']\n",
    "    \n",
    "    comm_date_set.add((comm, date))\n",
    "    \n",
    "data.apply(helper, axis = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairings = set()\n",
    "\n",
    "for city in data['Key Communities'].unique():\n",
    "    for days in pd.date_range(end = '2019-11-30', periods = 91):\n",
    "        pairings.add((city, days))\n",
    "\n",
    "pairings -= comm_date_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_on = []\n",
    "for pair in pairings:\n",
    "    power_ = {}\n",
    "    power_['Key Communities'] = pair[0]\n",
    "    power_['Start Date'] = pair[1]\n",
    "    power_on.append(power_)\n",
    "add_data = pd.DataFrame(power_on)\n",
    "add_data['Shutoff'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = pickle.load(open('locations','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = add_data.merge(locations, on = 'Key Communities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data['End Date'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(add_data,open('add_data','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = pickle.load(open('add_data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pd.concat([data, add_data], ignore_index = True, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrape weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "from IPython.core.display import display, HTML\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'User-agent': 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.17 Safari/537.36'}\n"
     ]
    }
   ],
   "source": [
    "ua = UserAgent()\n",
    "user_agent = {'User-agent': ua.random}\n",
    "print(user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(row):\n",
    "    index = row.name\n",
    "    \n",
    "    darksky_key = <PUT KEY HERE>\n",
    "    TIMESTAMP = str(row['Start Date'].timestamp()).split('.',1)[0]\n",
    "    LAT = str(row['Latitude'])\n",
    "    LONG = str(row['Longitude'])\n",
    "    darksky_url = 'https://api.darksky.net/forecast/' + darksky_key + '/' + LAT + ',' + LONG + ',' + TIMESTAMP + '?exclude=currently,flags'\n",
    "    \n",
    "    response = requests.get(darksky_url, headers = user_agent)\n",
    "    weather_data = response.json()\n",
    "\n",
    "    total_data.loc[index, 'High'] = weather_data['daily']['data'][0]['temperatureHigh']\n",
    "    total_data.loc[index, 'Low'] = weather_data['daily']['data'][0]['temperatureLow']\n",
    "    total_data.loc[index, 'Humidity'] = weather_data['daily']['data'][0]['humidity']\n",
    "    total_data.loc[index, 'Dewpoint'] = weather_data['daily']['data'][0]['dewPoint']\n",
    "    total_data.loc[index, 'Pressure'] = weather_data['daily']['data'][0]['pressure']\n",
    "    total_data.loc[index, 'Windspeed'] = weather_data['daily']['data'][0]['windSpeed']\n",
    "    total_data.loc[index, 'Windgust'] = weather_data['daily']['data'][0]['windGust']\n",
    "    total_data.loc[index, 'Cover'] = weather_data['daily']['data'][0]['cloudCover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data.apply(weather, axis = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(total_data,open('total_data','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pickle.load(open('total_data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIX MY DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data = pickle.load(open('total_data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_by_day = total_data.loc[:, ['Key Communities', 'Start Date', 'High', 'Low', 'Humidity', 'Dewpoint', 'Pressure',\n",
    "       'Windspeed', 'Windgust', 'Cover']].copy(deep=True)\n",
    "\n",
    "target_df = total_data.loc[total_data['Shutoff']==True, ['Key Communities', 'Start Date', 'Shutoff', 'Latitude', 'Longitude',\n",
    "       'End Date']].copy(deep=True)\n",
    "\n",
    "other_data = total_data.loc[total_data['Shutoff']==False, ['Key Communities', 'Start Date', 'Shutoff', 'Latitude', 'Longitude',\n",
    "       ]].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df['Day'] = target_df.apply(lambda row: pd.date_range(row['Start Date'], row['End Date']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = target_df.explode('Day').reset_index() \\\n",
    "        .drop(columns=['index', 'Start Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data = other_data.rename(columns={'Start Date': 'Day'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_and_other = pd.merge(target_df,other_data, on=['Key Communities', 'Day', 'Latitude', 'Longitude'], how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4643, 6), (27468, 5), (30811, 7)]\n"
     ]
    }
   ],
   "source": [
    "#it worked!! removed false shutoff rows that overlapped with target data\n",
    "print([target_df.shape, other_data.shape, target_and_other.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_target_df = target_and_other[target_and_other['Shutoff_x'].isnull()]\n",
    "add_to_target_df = add_to_target_df.drop(columns = ['Shutoff_x']) \\\n",
    "                    .rename(columns = {'Shutoff_y':'Shutoff'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([target_df, add_to_target_df], sort=False).drop(columns=['End Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_by_day = weather_by_day.rename(columns = {'Start Date':'Day'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_weather = df_final[df_final['Day']=='2019-06-09T00:00:00.000000000'] \\\n",
    "                    .drop_duplicates() \\\n",
    "                    .drop(columns = 'Shutoff') \\\n",
    "                    .rename(columns = {'Day': 'Start Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather2(row):\n",
    "    index = row.name\n",
    "    \n",
    "    darksky_key = <PUT KEY HERE>\n",
    "    TIMESTAMP = str(row['Start Date'].timestamp()).split('.',1)[0]\n",
    "    LAT = str(row['Latitude'])\n",
    "    LONG = str(row['Longitude'])\n",
    "    darksky_url = 'https://api.darksky.net/forecast/' + darksky_key + '/' + LAT + ',' + LONG + ',' + TIMESTAMP + '?exclude=currently,flags'\n",
    "    \n",
    "    response = requests.get(darksky_url, headers = user_agent)\n",
    "    weather_data = response.json()\n",
    "\n",
    "    missing_weather.loc[index, 'High'] = weather_data['daily']['data'][0]['temperatureHigh']\n",
    "    missing_weather.loc[index, 'Low'] = weather_data['daily']['data'][0]['temperatureLow']\n",
    "    missing_weather.loc[index, 'Humidity'] = weather_data['daily']['data'][0]['humidity']\n",
    "    missing_weather.loc[index, 'Dewpoint'] = weather_data['daily']['data'][0]['dewPoint']\n",
    "    missing_weather.loc[index, 'Pressure'] = weather_data['daily']['data'][0]['pressure']\n",
    "    missing_weather.loc[index, 'Windspeed'] = weather_data['daily']['data'][0]['windSpeed']\n",
    "    missing_weather.loc[index, 'Windgust'] = weather_data['daily']['data'][0]['windGust']\n",
    "    missing_weather.loc[index, 'Cover'] = weather_data['daily']['data'][0]['cloudCover']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adds community+day combos to weather_data\n",
    "missing_weather.apply(weather2, axis = 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Key Communities</th>\n",
       "      <th>Day</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Dewpoint</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Windspeed</th>\n",
       "      <th>Windgust</th>\n",
       "      <th>Cover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bangor</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>82.79</td>\n",
       "      <td>66.98</td>\n",
       "      <td>0.23</td>\n",
       "      <td>32.03</td>\n",
       "      <td>1016.4</td>\n",
       "      <td>7.22</td>\n",
       "      <td>25.28</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chico</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>88.69</td>\n",
       "      <td>63.99</td>\n",
       "      <td>0.21</td>\n",
       "      <td>28.17</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>9.15</td>\n",
       "      <td>19.56</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Oroville</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>88.58</td>\n",
       "      <td>65.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>29.63</td>\n",
       "      <td>1016.8</td>\n",
       "      <td>9.08</td>\n",
       "      <td>19.93</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Magalia</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>77.57</td>\n",
       "      <td>65.96</td>\n",
       "      <td>0.21</td>\n",
       "      <td>24.30</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>8.76</td>\n",
       "      <td>29.20</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Butte Meadows</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>69.68</td>\n",
       "      <td>52.96</td>\n",
       "      <td>0.27</td>\n",
       "      <td>20.13</td>\n",
       "      <td>1019.3</td>\n",
       "      <td>9.28</td>\n",
       "      <td>26.44</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Paradise</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>83.09</td>\n",
       "      <td>68.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>25.49</td>\n",
       "      <td>1017.7</td>\n",
       "      <td>6.03</td>\n",
       "      <td>21.50</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Berry Creek</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>78.33</td>\n",
       "      <td>62.85</td>\n",
       "      <td>0.24</td>\n",
       "      <td>26.72</td>\n",
       "      <td>1017.4</td>\n",
       "      <td>8.80</td>\n",
       "      <td>26.33</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Key Communities        Day   High    Low  Humidity  Dewpoint  Pressure  \\\n",
       "1            Bangor 2019-06-09  82.79  66.98      0.23     32.03    1016.4   \n",
       "19            Chico 2019-06-09  88.69  63.99      0.21     28.17    1017.6   \n",
       "76         Oroville 2019-06-09  88.58  65.70      0.21     29.63    1016.8   \n",
       "201         Magalia 2019-06-09  77.57  65.96      0.21     24.30    1018.0   \n",
       "228   Butte Meadows 2019-06-09  69.68  52.96      0.27     20.13    1019.3   \n",
       "232        Paradise 2019-06-09  83.09  68.19      0.20     25.49    1017.7   \n",
       "321     Berry Creek 2019-06-09  78.33  62.85      0.24     26.72    1017.4   \n",
       "\n",
       "     Windspeed  Windgust  Cover  \n",
       "1         7.22     25.28   0.12  \n",
       "19        9.15     19.56   0.11  \n",
       "76        9.08     19.93   0.11  \n",
       "201       8.76     29.20   0.12  \n",
       "228       9.28     26.44   0.13  \n",
       "232       6.03     21.50   0.11  \n",
       "321       8.80     26.33   0.12  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_weather = missing_weather.rename(columns = {'Start Date':'Day'}) \\\n",
    "                    .drop(columns = ['Latitude', 'Longitude'])\n",
    "missing_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather_by_day.shape = (28929, 10)\n",
    "weather_by_day = pd.concat([weather_by_day, missing_weather], sort=False)\n",
    "#weather_by_day.shape = (28936, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28044, 10)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_by_day = weather_by_day.drop_duplicates(['Key Communities','Day'],keep= 'last')\n",
    "weather_by_day.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.merge(df_final, weather_by_day, on = ['Key Communities', 'Day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data['Shutoff'] = model_data['Shutoff'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_data,open('model_data','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
